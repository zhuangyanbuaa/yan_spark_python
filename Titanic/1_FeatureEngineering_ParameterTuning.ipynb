{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate feature engineering process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define necessary functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for imputing missing values on each group\n",
    "# group by key: groupby_columns\n",
    "# column to be imputed: impute_column\n",
    "\n",
    "# group_by_imputer = GroupByImputer(strategy=\"median\")\n",
    "# group_by_imputer.fit(data_train, groupby_columns=['Pclass', 'Sex'], impute_column='Age')\n",
    "# data_train = group_by_imputer.transform(data_train)\n",
    "# data_test = group_by_imputer.transform(data_test)\n",
    "\n",
    "class GroupByImputer():\n",
    "    def __init__(self, missing_values=\"NaN\", strategy=\"mean\", \n",
    "                 axis=0, verbose=0, copy=True):\n",
    "        self.missing_values = missing_values\n",
    "        self.strategy = strategy\n",
    "        self.axis = axis\n",
    "        self.verbose = verbose\n",
    "        self.copy = copy\n",
    "    \n",
    "    def fit(self, X, groupby_columns, impute_column, y=None):\n",
    "        \n",
    "        self.group_by_imputers_ = X.groupby(groupby_columns).apply(lambda x: self._imputer_fit(x, impute_column, self.strategy))\n",
    "        \n",
    "        self.groupby_columns_ = groupby_columns\n",
    "        self.impute_column_ = impute_column\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _imputer_fit(self, x, impute_column, strategy):\n",
    "        \n",
    "        if data_train[impute_column].dtype == np.dtype('O'): \n",
    "            # object string\n",
    "            imputer = x[impute_column].value_counts().index[0]\n",
    "        else:\n",
    "            # int or float\n",
    "            imputer = preprocessing.Imputer(strategy=strategy)\n",
    "            imputer.fit(x[[impute_column]])\n",
    "        \n",
    "        return imputer\n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.groupby(self.groupby_columns_).apply(lambda x: self._imputer_transform(x))\n",
    "        return X\n",
    "    \n",
    "    def _imputer_transform(self, x):\n",
    "        \n",
    "        index = x.name\n",
    "        imputer = self.group_by_imputers_[index]\n",
    "        \n",
    "        if data_train[self.impute_column_].dtype == np.dtype('O'): \n",
    "            # object string\n",
    "            x[[self.impute_column_]] = x[[self.impute_column_]].fillna(imputer)\n",
    "            \n",
    "        else:\n",
    "            # int or float\n",
    "            x[[self.impute_column_]] = imputer.transform(x[[self.impute_column_]])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('./data/train.csv')\n",
    "data_test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>891</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Johnson, Miss. Eleanor Ileen</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>347082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>179.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.854200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>713.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.687500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PassengerId    Survived      Pclass                          Name  \\\n",
       "count    891.000000  891.000000  891.000000                           891   \n",
       "unique          NaN         NaN         NaN                           891   \n",
       "top             NaN         NaN         NaN  Johnson, Miss. Eleanor Ileen   \n",
       "freq            NaN         NaN         NaN                             1   \n",
       "mean     446.000000    0.383838    2.308642                           NaN   \n",
       "std      257.353842    0.486592    0.836071                           NaN   \n",
       "min        1.000000    0.000000    1.000000                           NaN   \n",
       "20%      179.000000    0.000000    1.000000                           NaN   \n",
       "50%      446.000000    0.000000    3.000000                           NaN   \n",
       "80%      713.000000    1.000000    3.000000                           NaN   \n",
       "max      891.000000    1.000000    3.000000                           NaN   \n",
       "\n",
       "         Sex         Age       SibSp       Parch  Ticket        Fare    Cabin  \\\n",
       "count    891  714.000000  891.000000  891.000000     891  891.000000      204   \n",
       "unique     2         NaN         NaN         NaN     681         NaN      147   \n",
       "top     male         NaN         NaN         NaN  347082         NaN  B96 B98   \n",
       "freq     577         NaN         NaN         NaN       7         NaN        4   \n",
       "mean     NaN   29.699118    0.523008    0.381594     NaN   32.204208      NaN   \n",
       "std      NaN   14.526497    1.102743    0.806057     NaN   49.693429      NaN   \n",
       "min      NaN    0.420000    0.000000    0.000000     NaN    0.000000      NaN   \n",
       "20%      NaN   19.000000    0.000000    0.000000     NaN    7.854200      NaN   \n",
       "50%      NaN   28.000000    0.000000    0.000000     NaN   14.454200      NaN   \n",
       "80%      NaN   41.000000    1.000000    1.000000     NaN   39.687500      NaN   \n",
       "max      NaN   80.000000    8.000000    6.000000     NaN  512.329200      NaN   \n",
       "\n",
       "       Embarked  \n",
       "count       889  \n",
       "unique        3  \n",
       "top           S  \n",
       "freq        644  \n",
       "mean        NaN  \n",
       "std         NaN  \n",
       "min         NaN  \n",
       "20%         NaN  \n",
       "50%         NaN  \n",
       "80%         NaN  \n",
       "max         NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.describe(percentiles=[0.2,0.8], include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: MICE implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age         177\n",
      "Cabin       687\n",
      "Embarked      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# columns with missing values\n",
    "columns_with_missing_values = list(data_train.columns[data_train.isnull().any()])\n",
    "# number of missing values on each column\n",
    "print(data_train[columns_with_missing_values].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute missing values with median value on each (Pclass, Sex) group for Age column\n",
    "age_group_by_imputer = GroupByImputer(strategy=\"median\")\n",
    "age_group_by_imputer.fit(data_train, groupby_columns=['Pclass', 'Sex'], impute_column='Age')\n",
    "data_train = age_group_by_imputer.transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GroupByImputer at 0x7f17e3fa2828>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impute missing values with median value on each (Pclass, Embarked) group for Fare column\n",
    "fare_group_by_imputer = GroupByImputer(strategy=\"median\")\n",
    "fare_group_by_imputer.fit(data_train, groupby_columns=['Pclass', 'Embarked'], impute_column='Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute missing values for Embarked column\n",
    "# Hypothesis: Fare is positive correlated with Pclass and Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <th>Embarked</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>C</th>\n",
       "      <td>78.2667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>90.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>52.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>C</th>\n",
       "      <td>24.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>12.3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>13.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>C</th>\n",
       "      <td>7.8958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>7.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Fare\n",
       "Pclass Embarked         \n",
       "1      C         78.2667\n",
       "       Q         90.0000\n",
       "       S         52.0000\n",
       "2      C         24.0000\n",
       "       Q         12.3500\n",
       "       S         13.5000\n",
       "3      C          7.8958\n",
       "       Q          7.7500\n",
       "       S          8.0500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[['Pclass', 'Embarked', 'Fare']].groupby(['Pclass', 'Embarked']).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Icard, Miss. Amelie</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113572</td>\n",
       "      <td>80.0</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>830</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Stone, Mrs. George Nelson (Martha Evelyn)</td>\n",
       "      <td>female</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113572</td>\n",
       "      <td>80.0</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                                       Name  \\\n",
       "61            62         1       1                        Icard, Miss. Amelie   \n",
       "829          830         1       1  Stone, Mrs. George Nelson (Martha Evelyn)   \n",
       "\n",
       "        Sex   Age  SibSp  Parch  Ticket  Fare Cabin Embarked  \n",
       "61   female  38.0      0      0  113572  80.0   B28      NaN  \n",
       "829  female  62.0      0      0  113572  80.0   B28      NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.loc[data_train['Embarked'].isnull(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Pclass 1, Fare 80 is nearest to Fare 78.2667, which Embarked is C\n",
    "data_train.loc[data_train['Embarked'].isnull(), 'Embarked'] = 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Handling on Cabin: 0 for null and 1 for not null\n",
    "# data_train['Cabin_flg'] = 0\n",
    "# data_train.loc[data_train.Cabin.notnull(), 'Cabin_flg'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Convert SibSp with more than 2 into 'more_than_2'\n",
    "# data_train['SibSp_flg'] = data_train['SibSp']\n",
    "# data_train.loc[data_train['SibSp'] >= 2, 'SibSp_flg'] = 'more_than_2'\n",
    "# data_train['SibSp_flg'] = data_train['SibSp_flg'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert Parch with more than 2 into 'more_than_2'\n",
    "# data_train['Parch_flg'] = data_train['Parch']\n",
    "# data_train.loc[data_train['Parch'] >= 2, 'Parch_flg'] = 'more_than_2'\n",
    "# data_train['Parch_flg'] = data_train['Parch_flg'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Title from Name\n",
    "def generate_title(data, column='Name', new_column='Title', main_levels=['Mr', 'Master', 'Miss', 'Mrs']):\n",
    "    # extract title from name\n",
    "    data[new_column] = data[column].str.extract(',\\s*([^\\.]*)\\s*\\.', expand=False)\n",
    "    # Mr: Mr; Master: Master; Miss: Miss; Mrs: Mrs; Others: Rare\n",
    "    data[new_column] = data[new_column].str.replace('|'.join(list(set(data[new_column].unique()) - set(main_levels))), 'Rare')\n",
    "    return data\n",
    "\n",
    "data_train = generate_title(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine SibSp and Parch and passenger as Fsize(Family Size)\n",
    "data_train['Fsize'] = data_train['SibSp'] + data_train['Parch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature to divide Age into 'child' and 'adult'\n",
    "data_train['Age_New'] = 'adult'\n",
    "data_train.loc[data_train['Age'] < 18, 'Age_New'] = 'child'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature list\n",
    "numerical_variables = ['Fsize', 'Fare']\n",
    "categorical_variables = ['Pclass', 'Sex', 'Embarked', 'Title', 'Age_New']\n",
    "# features = numerical_variables + categorical_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OneHotEncoder\n",
    "ohe = ce.one_hot.OneHotEncoder(cols=categorical_variables)\n",
    "ohe.fit(data_train)\n",
    "# Transform training data\n",
    "data_train = ohe.transform(data_train)\n",
    "# Remove columns with name containing '-1'(all 0)\n",
    "data_train = data_train[[c for c in data_train.columns if '-1' not in c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy categorical variables created by OneHotEncoding\n",
    "dummy_categorical_variables = list()\n",
    "for categorical_variable in categorical_variables:\n",
    "    dummy_categorical_variables = dummy_categorical_variables + [c for c in data_train.columns if categorical_variable in c]\n",
    "    \n",
    "features = dummy_categorical_variables + numerical_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = data_train[features]\n",
    "y_train = data_train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = OrderedDict()\n",
    "classifiers['Logistic Regression'] = LogisticRegression()\n",
    "classifiers['Decision Tree'] = DecisionTreeClassifier()\n",
    "classifiers['Random Forest'] = RandomForestClassifier()\n",
    "classifiers['AdaBoost'] = AdaBoostClassifier()\n",
    "classifiers['Gradient Boosting'] = GradientBoostingClassifier()\n",
    "classifiers['Naive Bayes'] = GaussianNB()\n",
    "classifiers['XGBoost'] = xgb.XGBClassifier()\n",
    "# 各学習器をCVでパフォーマンスを出力\n",
    "cv_result_df = pd.DataFrame(columns=['classifier', 'cv_scores_mean'])\n",
    "for clf_name, classifier in classifiers.items():\n",
    "    cv_scores = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    cv_result_df = cv_result_df.append({cv_result_df.columns.values[0]: clf_name,\\\n",
    "                                        cv_result_df.columns.values[1]: cv_scores.mean()},\\\n",
    "                                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>cv_scores_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.824943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.806997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.804806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.818233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.824981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.782290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.807016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            classifier  cv_scores_mean\n",
       "0  Logistic Regression        0.824943\n",
       "1        Decision Tree        0.806997\n",
       "2        Random Forest        0.804806\n",
       "3             AdaBoost        0.818233\n",
       "4    Gradient Boosting        0.824981\n",
       "5          Naive Bayes        0.782290\n",
       "6              XGBoost        0.807016"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [100, 110, 120, 130, 140, 150, 160, 170, 180, 190], 'max_depth': [3, 4, 5, 6, 7, 8, 9], 'subsample': [0.8, 0.9, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(learning_rate=0.1, silent=True, objective='binary:logistic')\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': list(range(100, 200, 10)),\n",
    "    'max_depth': list(range(3, 10, 1)),\n",
    "    'subsample': [0.8, 0.9, 1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = clf, param_grid = param_grid, scoring='accuracy', cv=5)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.82828, std: 0.02808, params: {'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8},\n",
       "  mean: 0.82267, std: 0.02800, params: {'max_depth': 3, 'n_estimators': 100, 'subsample': 0.9},\n",
       "  mean: 0.80696, std: 0.03328, params: {'max_depth': 3, 'n_estimators': 100, 'subsample': 1},\n",
       "  mean: 0.82604, std: 0.02751, params: {'max_depth': 3, 'n_estimators': 110, 'subsample': 0.8},\n",
       "  mean: 0.81930, std: 0.03019, params: {'max_depth': 3, 'n_estimators': 110, 'subsample': 0.9},\n",
       "  mean: 0.81481, std: 0.03089, params: {'max_depth': 3, 'n_estimators': 110, 'subsample': 1},\n",
       "  mean: 0.82492, std: 0.02363, params: {'max_depth': 3, 'n_estimators': 120, 'subsample': 0.8},\n",
       "  mean: 0.82379, std: 0.02863, params: {'max_depth': 3, 'n_estimators': 120, 'subsample': 0.9},\n",
       "  mean: 0.81369, std: 0.03155, params: {'max_depth': 3, 'n_estimators': 120, 'subsample': 1},\n",
       "  mean: 0.82941, std: 0.02561, params: {'max_depth': 3, 'n_estimators': 130, 'subsample': 0.8},\n",
       "  mean: 0.82155, std: 0.03134, params: {'max_depth': 3, 'n_estimators': 130, 'subsample': 0.9},\n",
       "  mean: 0.81257, std: 0.02995, params: {'max_depth': 3, 'n_estimators': 130, 'subsample': 1},\n",
       "  mean: 0.82941, std: 0.02815, params: {'max_depth': 3, 'n_estimators': 140, 'subsample': 0.8},\n",
       "  mean: 0.81930, std: 0.03114, params: {'max_depth': 3, 'n_estimators': 140, 'subsample': 0.9},\n",
       "  mean: 0.81369, std: 0.03091, params: {'max_depth': 3, 'n_estimators': 140, 'subsample': 1},\n",
       "  mean: 0.82828, std: 0.02947, params: {'max_depth': 3, 'n_estimators': 150, 'subsample': 0.8},\n",
       "  mean: 0.82043, std: 0.03069, params: {'max_depth': 3, 'n_estimators': 150, 'subsample': 0.9},\n",
       "  mean: 0.81594, std: 0.02929, params: {'max_depth': 3, 'n_estimators': 150, 'subsample': 1},\n",
       "  mean: 0.82604, std: 0.02527, params: {'max_depth': 3, 'n_estimators': 160, 'subsample': 0.8},\n",
       "  mean: 0.82155, std: 0.03255, params: {'max_depth': 3, 'n_estimators': 160, 'subsample': 0.9},\n",
       "  mean: 0.81818, std: 0.03176, params: {'max_depth': 3, 'n_estimators': 160, 'subsample': 1},\n",
       "  mean: 0.82828, std: 0.03107, params: {'max_depth': 3, 'n_estimators': 170, 'subsample': 0.8},\n",
       "  mean: 0.82155, std: 0.03255, params: {'max_depth': 3, 'n_estimators': 170, 'subsample': 0.9},\n",
       "  mean: 0.81818, std: 0.03176, params: {'max_depth': 3, 'n_estimators': 170, 'subsample': 1},\n",
       "  mean: 0.82716, std: 0.02850, params: {'max_depth': 3, 'n_estimators': 180, 'subsample': 0.8},\n",
       "  mean: 0.81930, std: 0.03504, params: {'max_depth': 3, 'n_estimators': 180, 'subsample': 0.9},\n",
       "  mean: 0.82043, std: 0.03090, params: {'max_depth': 3, 'n_estimators': 180, 'subsample': 1},\n",
       "  mean: 0.82828, std: 0.02791, params: {'max_depth': 3, 'n_estimators': 190, 'subsample': 0.8},\n",
       "  mean: 0.82043, std: 0.03463, params: {'max_depth': 3, 'n_estimators': 190, 'subsample': 0.9},\n",
       "  mean: 0.82155, std: 0.03040, params: {'max_depth': 3, 'n_estimators': 190, 'subsample': 1},\n",
       "  mean: 0.82267, std: 0.02844, params: {'max_depth': 4, 'n_estimators': 100, 'subsample': 0.8},\n",
       "  mean: 0.82267, std: 0.03510, params: {'max_depth': 4, 'n_estimators': 100, 'subsample': 0.9},\n",
       "  mean: 0.82043, std: 0.02365, params: {'max_depth': 4, 'n_estimators': 100, 'subsample': 1},\n",
       "  mean: 0.82492, std: 0.02297, params: {'max_depth': 4, 'n_estimators': 110, 'subsample': 0.8},\n",
       "  mean: 0.82267, std: 0.03510, params: {'max_depth': 4, 'n_estimators': 110, 'subsample': 0.9},\n",
       "  mean: 0.82043, std: 0.02423, params: {'max_depth': 4, 'n_estimators': 110, 'subsample': 1},\n",
       "  mean: 0.82941, std: 0.03360, params: {'max_depth': 4, 'n_estimators': 120, 'subsample': 0.8},\n",
       "  mean: 0.82492, std: 0.03262, params: {'max_depth': 4, 'n_estimators': 120, 'subsample': 0.9},\n",
       "  mean: 0.81930, std: 0.02305, params: {'max_depth': 4, 'n_estimators': 120, 'subsample': 1},\n",
       "  mean: 0.82941, std: 0.03358, params: {'max_depth': 4, 'n_estimators': 130, 'subsample': 0.8},\n",
       "  mean: 0.82941, std: 0.03074, params: {'max_depth': 4, 'n_estimators': 130, 'subsample': 0.9},\n",
       "  mean: 0.82043, std: 0.02151, params: {'max_depth': 4, 'n_estimators': 130, 'subsample': 1},\n",
       "  mean: 0.83053, std: 0.03105, params: {'max_depth': 4, 'n_estimators': 140, 'subsample': 0.8},\n",
       "  mean: 0.82828, std: 0.02791, params: {'max_depth': 4, 'n_estimators': 140, 'subsample': 0.9},\n",
       "  mean: 0.82155, std: 0.02271, params: {'max_depth': 4, 'n_estimators': 140, 'subsample': 1},\n",
       "  mean: 0.82828, std: 0.03138, params: {'max_depth': 4, 'n_estimators': 150, 'subsample': 0.8},\n",
       "  mean: 0.82716, std: 0.02833, params: {'max_depth': 4, 'n_estimators': 150, 'subsample': 0.9},\n",
       "  mean: 0.82267, std: 0.02361, params: {'max_depth': 4, 'n_estimators': 150, 'subsample': 1},\n",
       "  mean: 0.82604, std: 0.02979, params: {'max_depth': 4, 'n_estimators': 160, 'subsample': 0.8},\n",
       "  mean: 0.82828, std: 0.02751, params: {'max_depth': 4, 'n_estimators': 160, 'subsample': 0.9},\n",
       "  mean: 0.82267, std: 0.02361, params: {'max_depth': 4, 'n_estimators': 160, 'subsample': 1},\n",
       "  mean: 0.82379, std: 0.02877, params: {'max_depth': 4, 'n_estimators': 170, 'subsample': 0.8},\n",
       "  mean: 0.82604, std: 0.02908, params: {'max_depth': 4, 'n_estimators': 170, 'subsample': 0.9},\n",
       "  mean: 0.82267, std: 0.02361, params: {'max_depth': 4, 'n_estimators': 170, 'subsample': 1},\n",
       "  mean: 0.82267, std: 0.02942, params: {'max_depth': 4, 'n_estimators': 180, 'subsample': 0.8},\n",
       "  mean: 0.82828, std: 0.02933, params: {'max_depth': 4, 'n_estimators': 180, 'subsample': 0.9},\n",
       "  mean: 0.82155, std: 0.01884, params: {'max_depth': 4, 'n_estimators': 180, 'subsample': 1},\n",
       "  mean: 0.82716, std: 0.02868, params: {'max_depth': 4, 'n_estimators': 190, 'subsample': 0.8},\n",
       "  mean: 0.82716, std: 0.03052, params: {'max_depth': 4, 'n_estimators': 190, 'subsample': 0.9},\n",
       "  mean: 0.82043, std: 0.02296, params: {'max_depth': 4, 'n_estimators': 190, 'subsample': 1},\n",
       "  mean: 0.82379, std: 0.02617, params: {'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8},\n",
       "  mean: 0.82941, std: 0.02551, params: {'max_depth': 5, 'n_estimators': 100, 'subsample': 0.9},\n",
       "  mean: 0.82267, std: 0.02547, params: {'max_depth': 5, 'n_estimators': 100, 'subsample': 1},\n",
       "  mean: 0.81930, std: 0.02118, params: {'max_depth': 5, 'n_estimators': 110, 'subsample': 0.8},\n",
       "  mean: 0.83277, std: 0.03122, params: {'max_depth': 5, 'n_estimators': 110, 'subsample': 0.9},\n",
       "  mean: 0.82604, std: 0.02526, params: {'max_depth': 5, 'n_estimators': 110, 'subsample': 1},\n",
       "  mean: 0.82492, std: 0.02481, params: {'max_depth': 5, 'n_estimators': 120, 'subsample': 0.8},\n",
       "  mean: 0.83165, std: 0.02927, params: {'max_depth': 5, 'n_estimators': 120, 'subsample': 0.9},\n",
       "  mean: 0.82379, std: 0.02369, params: {'max_depth': 5, 'n_estimators': 120, 'subsample': 1},\n",
       "  mean: 0.82604, std: 0.02677, params: {'max_depth': 5, 'n_estimators': 130, 'subsample': 0.8},\n",
       "  mean: 0.83053, std: 0.02736, params: {'max_depth': 5, 'n_estimators': 130, 'subsample': 0.9},\n",
       "  mean: 0.82492, std: 0.02543, params: {'max_depth': 5, 'n_estimators': 130, 'subsample': 1},\n",
       "  mean: 0.82379, std: 0.02802, params: {'max_depth': 5, 'n_estimators': 140, 'subsample': 0.8},\n",
       "  mean: 0.83053, std: 0.02907, params: {'max_depth': 5, 'n_estimators': 140, 'subsample': 0.9},\n",
       "  mean: 0.82492, std: 0.02543, params: {'max_depth': 5, 'n_estimators': 140, 'subsample': 1},\n",
       "  mean: 0.82492, std: 0.02686, params: {'max_depth': 5, 'n_estimators': 150, 'subsample': 0.8},\n",
       "  mean: 0.82941, std: 0.02717, params: {'max_depth': 5, 'n_estimators': 150, 'subsample': 0.9},\n",
       "  mean: 0.82492, std: 0.02231, params: {'max_depth': 5, 'n_estimators': 150, 'subsample': 1},\n",
       "  mean: 0.82492, std: 0.02992, params: {'max_depth': 5, 'n_estimators': 160, 'subsample': 0.8},\n",
       "  mean: 0.83053, std: 0.02777, params: {'max_depth': 5, 'n_estimators': 160, 'subsample': 0.9},\n",
       "  mean: 0.82379, std: 0.02156, params: {'max_depth': 5, 'n_estimators': 160, 'subsample': 1},\n",
       "  mean: 0.82267, std: 0.03118, params: {'max_depth': 5, 'n_estimators': 170, 'subsample': 0.8},\n",
       "  mean: 0.82941, std: 0.02595, params: {'max_depth': 5, 'n_estimators': 170, 'subsample': 0.9},\n",
       "  mean: 0.82492, std: 0.02231, params: {'max_depth': 5, 'n_estimators': 170, 'subsample': 1},\n",
       "  mean: 0.82492, std: 0.03149, params: {'max_depth': 5, 'n_estimators': 180, 'subsample': 0.8},\n",
       "  mean: 0.82828, std: 0.02709, params: {'max_depth': 5, 'n_estimators': 180, 'subsample': 0.9},\n",
       "  mean: 0.82604, std: 0.02410, params: {'max_depth': 5, 'n_estimators': 180, 'subsample': 1},\n",
       "  mean: 0.82604, std: 0.03043, params: {'max_depth': 5, 'n_estimators': 190, 'subsample': 0.8},\n",
       "  mean: 0.82716, std: 0.02752, params: {'max_depth': 5, 'n_estimators': 190, 'subsample': 0.9},\n",
       "  mean: 0.82492, std: 0.02346, params: {'max_depth': 5, 'n_estimators': 190, 'subsample': 1},\n",
       "  mean: 0.82604, std: 0.02724, params: {'max_depth': 6, 'n_estimators': 100, 'subsample': 0.8},\n",
       "  mean: 0.82604, std: 0.03444, params: {'max_depth': 6, 'n_estimators': 100, 'subsample': 0.9},\n",
       "  mean: 0.82716, std: 0.01963, params: {'max_depth': 6, 'n_estimators': 100, 'subsample': 1},\n",
       "  mean: 0.82492, std: 0.02767, params: {'max_depth': 6, 'n_estimators': 110, 'subsample': 0.8},\n",
       "  mean: 0.82716, std: 0.03191, params: {'max_depth': 6, 'n_estimators': 110, 'subsample': 0.9},\n",
       "  mean: 0.82716, std: 0.01770, params: {'max_depth': 6, 'n_estimators': 110, 'subsample': 1},\n",
       "  mean: 0.82828, std: 0.02883, params: {'max_depth': 6, 'n_estimators': 120, 'subsample': 0.8},\n",
       "  mean: 0.82604, std: 0.03330, params: {'max_depth': 6, 'n_estimators': 120, 'subsample': 0.9},\n",
       "  mean: 0.82716, std: 0.01770, params: {'max_depth': 6, 'n_estimators': 120, 'subsample': 1},\n",
       "  mean: 0.82716, std: 0.02858, params: {'max_depth': 6, 'n_estimators': 130, 'subsample': 0.8},\n",
       "  mean: 0.82379, std: 0.03360, params: {'max_depth': 6, 'n_estimators': 130, 'subsample': 0.9},\n",
       "  mean: 0.82492, std: 0.01962, params: {'max_depth': 6, 'n_estimators': 130, 'subsample': 1},\n",
       "  mean: 0.82828, std: 0.02731, params: {'max_depth': 6, 'n_estimators': 140, 'subsample': 0.8},\n",
       "  mean: 0.82604, std: 0.03142, params: {'max_depth': 6, 'n_estimators': 140, 'subsample': 0.9},\n",
       "  mean: 0.82379, std: 0.02118, params: {'max_depth': 6, 'n_estimators': 140, 'subsample': 1},\n",
       "  mean: 0.82716, std: 0.02439, params: {'max_depth': 6, 'n_estimators': 150, 'subsample': 0.8},\n",
       "  mean: 0.82492, std: 0.03245, params: {'max_depth': 6, 'n_estimators': 150, 'subsample': 0.9},\n",
       "  mean: 0.82492, std: 0.01962, params: {'max_depth': 6, 'n_estimators': 150, 'subsample': 1},\n",
       "  mean: 0.82828, std: 0.02731, params: {'max_depth': 6, 'n_estimators': 160, 'subsample': 0.8},\n",
       "  mean: 0.82604, std: 0.03236, params: {'max_depth': 6, 'n_estimators': 160, 'subsample': 0.9},\n",
       "  mean: 0.82492, std: 0.01799, params: {'max_depth': 6, 'n_estimators': 160, 'subsample': 1},\n",
       "  mean: 0.82828, std: 0.02883, params: {'max_depth': 6, 'n_estimators': 170, 'subsample': 0.8},\n",
       "  mean: 0.82716, std: 0.03148, params: {'max_depth': 6, 'n_estimators': 170, 'subsample': 0.9},\n",
       "  mean: 0.82604, std: 0.01915, params: {'max_depth': 6, 'n_estimators': 170, 'subsample': 1},\n",
       "  mean: 0.83053, std: 0.02635, params: {'max_depth': 6, 'n_estimators': 180, 'subsample': 0.8},\n",
       "  mean: 0.82379, std: 0.02926, params: {'max_depth': 6, 'n_estimators': 180, 'subsample': 0.9},\n",
       "  mean: 0.82492, std: 0.01988, params: {'max_depth': 6, 'n_estimators': 180, 'subsample': 1},\n",
       "  mean: 0.83389, std: 0.02635, params: {'max_depth': 6, 'n_estimators': 190, 'subsample': 0.8},\n",
       "  mean: 0.82604, std: 0.02738, params: {'max_depth': 6, 'n_estimators': 190, 'subsample': 0.9},\n",
       "  mean: 0.82604, std: 0.01915, params: {'max_depth': 6, 'n_estimators': 190, 'subsample': 1},\n",
       "  mean: 0.82379, std: 0.02887, params: {'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8},\n",
       "  mean: 0.82604, std: 0.02819, params: {'max_depth': 7, 'n_estimators': 100, 'subsample': 0.9},\n",
       "  mean: 0.82379, std: 0.02680, params: {'max_depth': 7, 'n_estimators': 100, 'subsample': 1},\n",
       "  mean: 0.82379, std: 0.02802, params: {'max_depth': 7, 'n_estimators': 110, 'subsample': 0.8},\n",
       "  mean: 0.82716, std: 0.02868, params: {'max_depth': 7, 'n_estimators': 110, 'subsample': 0.9},\n",
       "  mean: 0.82492, std: 0.02626, params: {'max_depth': 7, 'n_estimators': 110, 'subsample': 1},\n",
       "  mean: 0.82828, std: 0.02573, params: {'max_depth': 7, 'n_estimators': 120, 'subsample': 0.8},\n",
       "  mean: 0.82716, std: 0.02718, params: {'max_depth': 7, 'n_estimators': 120, 'subsample': 0.9},\n",
       "  mean: 0.82379, std: 0.02744, params: {'max_depth': 7, 'n_estimators': 120, 'subsample': 1},\n",
       "  mean: 0.82941, std: 0.02477, params: {'max_depth': 7, 'n_estimators': 130, 'subsample': 0.8},\n",
       "  mean: 0.82828, std: 0.02827, params: {'max_depth': 7, 'n_estimators': 130, 'subsample': 0.9},\n",
       "  mean: 0.82716, std: 0.02653, params: {'max_depth': 7, 'n_estimators': 130, 'subsample': 1},\n",
       "  mean: 0.82828, std: 0.02478, params: {'max_depth': 7, 'n_estimators': 140, 'subsample': 0.8},\n",
       "  mean: 0.82941, std: 0.02428, params: {'max_depth': 7, 'n_estimators': 140, 'subsample': 0.9},\n",
       "  mean: 0.82828, std: 0.02796, params: {'max_depth': 7, 'n_estimators': 140, 'subsample': 1},\n",
       "  mean: 0.82941, std: 0.02405, params: {'max_depth': 7, 'n_estimators': 150, 'subsample': 0.8},\n",
       "  mean: 0.82941, std: 0.02499, params: {'max_depth': 7, 'n_estimators': 150, 'subsample': 0.9},\n",
       "  mean: 0.82828, std: 0.02796, params: {'max_depth': 7, 'n_estimators': 150, 'subsample': 1},\n",
       "  mean: 0.82828, std: 0.02665, params: {'max_depth': 7, 'n_estimators': 160, 'subsample': 0.8},\n",
       "  mean: 0.82828, std: 0.02618, params: {'max_depth': 7, 'n_estimators': 160, 'subsample': 0.9},\n",
       "  mean: 0.82379, std: 0.02923, params: {'max_depth': 7, 'n_estimators': 160, 'subsample': 1},\n",
       "  mean: 0.83165, std: 0.02692, params: {'max_depth': 7, 'n_estimators': 170, 'subsample': 0.8},\n",
       "  mean: 0.82828, std: 0.02618, params: {'max_depth': 7, 'n_estimators': 170, 'subsample': 0.9},\n",
       "  mean: 0.82716, std: 0.02859, params: {'max_depth': 7, 'n_estimators': 170, 'subsample': 1},\n",
       "  mean: 0.83165, std: 0.02532, params: {'max_depth': 7, 'n_estimators': 180, 'subsample': 0.8},\n",
       "  mean: 0.82941, std: 0.02731, params: {'max_depth': 7, 'n_estimators': 180, 'subsample': 0.9},\n",
       "  mean: 0.82604, std: 0.02804, params: {'max_depth': 7, 'n_estimators': 180, 'subsample': 1},\n",
       "  mean: 0.83502, std: 0.02566, params: {'max_depth': 7, 'n_estimators': 190, 'subsample': 0.8},\n",
       "  mean: 0.82941, std: 0.02689, params: {'max_depth': 7, 'n_estimators': 190, 'subsample': 0.9},\n",
       "  mean: 0.82492, std: 0.02859, params: {'max_depth': 7, 'n_estimators': 190, 'subsample': 1},\n",
       "  mean: 0.82492, std: 0.02796, params: {'max_depth': 8, 'n_estimators': 100, 'subsample': 0.8},\n",
       "  mean: 0.82828, std: 0.03400, params: {'max_depth': 8, 'n_estimators': 100, 'subsample': 0.9},\n",
       "  mean: 0.82716, std: 0.02637, params: {'max_depth': 8, 'n_estimators': 100, 'subsample': 1},\n",
       "  mean: 0.82379, std: 0.02615, params: {'max_depth': 8, 'n_estimators': 110, 'subsample': 0.8},\n",
       "  mean: 0.82828, std: 0.03344, params: {'max_depth': 8, 'n_estimators': 110, 'subsample': 0.9},\n",
       "  mean: 0.82716, std: 0.02637, params: {'max_depth': 8, 'n_estimators': 110, 'subsample': 1},\n",
       "  mean: 0.82379, std: 0.02760, params: {'max_depth': 8, 'n_estimators': 120, 'subsample': 0.8},\n",
       "  mean: 0.82941, std: 0.02795, params: {'max_depth': 8, 'n_estimators': 120, 'subsample': 0.9},\n",
       "  mean: 0.82828, std: 0.03078, params: {'max_depth': 8, 'n_estimators': 120, 'subsample': 1},\n",
       "  mean: 0.82604, std: 0.02850, params: {'max_depth': 8, 'n_estimators': 130, 'subsample': 0.8},\n",
       "  mean: 0.82941, std: 0.02944, params: {'max_depth': 8, 'n_estimators': 130, 'subsample': 0.9},\n",
       "  mean: 0.82941, std: 0.03014, params: {'max_depth': 8, 'n_estimators': 130, 'subsample': 1},\n",
       "  mean: 0.82716, std: 0.02858, params: {'max_depth': 8, 'n_estimators': 140, 'subsample': 0.8},\n",
       "  mean: 0.83277, std: 0.02586, params: {'max_depth': 8, 'n_estimators': 140, 'subsample': 0.9},\n",
       "  mean: 0.83053, std: 0.02967, params: {'max_depth': 8, 'n_estimators': 140, 'subsample': 1},\n",
       "  mean: 0.83053, std: 0.02398, params: {'max_depth': 8, 'n_estimators': 150, 'subsample': 0.8},\n",
       "  mean: 0.83165, std: 0.02577, params: {'max_depth': 8, 'n_estimators': 150, 'subsample': 0.9},\n",
       "  mean: 0.83053, std: 0.02907, params: {'max_depth': 8, 'n_estimators': 150, 'subsample': 1},\n",
       "  mean: 0.82604, std: 0.02699, params: {'max_depth': 8, 'n_estimators': 160, 'subsample': 0.8},\n",
       "  mean: 0.83053, std: 0.02813, params: {'max_depth': 8, 'n_estimators': 160, 'subsample': 0.9},\n",
       "  mean: 0.82941, std: 0.02797, params: {'max_depth': 8, 'n_estimators': 160, 'subsample': 1},\n",
       "  mean: 0.82828, std: 0.02753, params: {'max_depth': 8, 'n_estimators': 170, 'subsample': 0.8},\n",
       "  mean: 0.83053, std: 0.02583, params: {'max_depth': 8, 'n_estimators': 170, 'subsample': 0.9},\n",
       "  mean: 0.82828, std: 0.02843, params: {'max_depth': 8, 'n_estimators': 170, 'subsample': 1},\n",
       "  mean: 0.82941, std: 0.02664, params: {'max_depth': 8, 'n_estimators': 180, 'subsample': 0.8},\n",
       "  mean: 0.82828, std: 0.02791, params: {'max_depth': 8, 'n_estimators': 180, 'subsample': 0.9},\n",
       "  mean: 0.82828, std: 0.02636, params: {'max_depth': 8, 'n_estimators': 180, 'subsample': 1},\n",
       "  mean: 0.82604, std: 0.02679, params: {'max_depth': 8, 'n_estimators': 190, 'subsample': 0.8},\n",
       "  mean: 0.82941, std: 0.02564, params: {'max_depth': 8, 'n_estimators': 190, 'subsample': 0.9},\n",
       "  mean: 0.82716, std: 0.02680, params: {'max_depth': 8, 'n_estimators': 190, 'subsample': 1},\n",
       "  mean: 0.82604, std: 0.02850, params: {'max_depth': 9, 'n_estimators': 100, 'subsample': 0.8},\n",
       "  mean: 0.83277, std: 0.02729, params: {'max_depth': 9, 'n_estimators': 100, 'subsample': 0.9},\n",
       "  mean: 0.82716, std: 0.02392, params: {'max_depth': 9, 'n_estimators': 100, 'subsample': 1},\n",
       "  mean: 0.82941, std: 0.02753, params: {'max_depth': 9, 'n_estimators': 110, 'subsample': 0.8},\n",
       "  mean: 0.82941, std: 0.02515, params: {'max_depth': 9, 'n_estimators': 110, 'subsample': 0.9},\n",
       "  mean: 0.82379, std: 0.02664, params: {'max_depth': 9, 'n_estimators': 110, 'subsample': 1},\n",
       "  mean: 0.82828, std: 0.02711, params: {'max_depth': 9, 'n_estimators': 120, 'subsample': 0.8},\n",
       "  mean: 0.83165, std: 0.02507, params: {'max_depth': 9, 'n_estimators': 120, 'subsample': 0.9},\n",
       "  mean: 0.82492, std: 0.02586, params: {'max_depth': 9, 'n_estimators': 120, 'subsample': 1},\n",
       "  mean: 0.83053, std: 0.02682, params: {'max_depth': 9, 'n_estimators': 130, 'subsample': 0.8},\n",
       "  mean: 0.83165, std: 0.02692, params: {'max_depth': 9, 'n_estimators': 130, 'subsample': 0.9},\n",
       "  mean: 0.82492, std: 0.02586, params: {'max_depth': 9, 'n_estimators': 130, 'subsample': 1},\n",
       "  mean: 0.83053, std: 0.02682, params: {'max_depth': 9, 'n_estimators': 140, 'subsample': 0.8},\n",
       "  mean: 0.83165, std: 0.02553, params: {'max_depth': 9, 'n_estimators': 140, 'subsample': 0.9},\n",
       "  mean: 0.82379, std: 0.02683, params: {'max_depth': 9, 'n_estimators': 140, 'subsample': 1},\n",
       "  mean: 0.82941, std: 0.02556, params: {'max_depth': 9, 'n_estimators': 150, 'subsample': 0.8},\n",
       "  mean: 0.83165, std: 0.02271, params: {'max_depth': 9, 'n_estimators': 150, 'subsample': 0.9},\n",
       "  mean: 0.82379, std: 0.02683, params: {'max_depth': 9, 'n_estimators': 150, 'subsample': 1},\n",
       "  mean: 0.82604, std: 0.02679, params: {'max_depth': 9, 'n_estimators': 160, 'subsample': 0.8},\n",
       "  mean: 0.83277, std: 0.02281, params: {'max_depth': 9, 'n_estimators': 160, 'subsample': 0.9},\n",
       "  mean: 0.82379, std: 0.02923, params: {'max_depth': 9, 'n_estimators': 160, 'subsample': 1},\n",
       "  mean: 0.82828, std: 0.02421, params: {'max_depth': 9, 'n_estimators': 170, 'subsample': 0.8},\n",
       "  mean: 0.83165, std: 0.02370, params: {'max_depth': 9, 'n_estimators': 170, 'subsample': 0.9},\n",
       "  mean: 0.82492, std: 0.02812, params: {'max_depth': 9, 'n_estimators': 170, 'subsample': 1},\n",
       "  mean: 0.83053, std: 0.02635, params: {'max_depth': 9, 'n_estimators': 180, 'subsample': 0.8},\n",
       "  mean: 0.83053, std: 0.02763, params: {'max_depth': 9, 'n_estimators': 180, 'subsample': 0.9},\n",
       "  mean: 0.82716, std: 0.02680, params: {'max_depth': 9, 'n_estimators': 180, 'subsample': 1},\n",
       "  mean: 0.82941, std: 0.02319, params: {'max_depth': 9, 'n_estimators': 190, 'subsample': 0.8},\n",
       "  mean: 0.82828, std: 0.02361, params: {'max_depth': 9, 'n_estimators': 190, 'subsample': 0.9},\n",
       "  mean: 0.82604, std: 0.02779, params: {'max_depth': 9, 'n_estimators': 190, 'subsample': 1}],\n",
       " {'max_depth': 7, 'n_estimators': 190, 'subsample': 0.8},\n",
       " 0.835016835016835)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.grid_scores_, grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m--------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   max_depth |   n_estimators |   subsample | \n",
      "    1 | 00m02s | \u001b[35m   0.82388\u001b[0m | \u001b[32m     9.4978\u001b[0m | \u001b[32m      185.2594\u001b[0m | \u001b[32m     0.9748\u001b[0m | \n",
      "    2 | 00m01s | \u001b[35m   0.82723\u001b[0m | \u001b[32m     9.0468\u001b[0m | \u001b[32m      133.6669\u001b[0m | \u001b[32m     0.5692\u001b[0m | \n",
      "    3 | 00m00s |    0.82722 |      3.5421 |       155.3604 |      0.5864 | \n",
      "    4 | 00m00s |    0.82611 |      3.0321 |       190.5833 |      0.6820 | \n",
      "    5 | 00m01s |    0.82722 |      7.5477 |       187.1834 |      0.6955 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m--------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   max_depth |   n_estimators |   subsample | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  1.50904054e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 55, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -6.02762752e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 54, 'nit': 4, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6 | 00m20s |    0.82161 |      3.0000 |       100.0000 |      0.5000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    7 | 00m15s | \u001b[35m   0.83174\u001b[0m | \u001b[32m     9.9479\u001b[0m | \u001b[32m      199.9642\u001b[0m | \u001b[32m     0.5329\u001b[0m | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-0.00019509]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 53, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8 | 00m13s |    0.82160 |      3.0033 |       125.0101 |      0.5152 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.98802372e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 49, 'nit': 4, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  6.59264333e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 45, 'nit': 3, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9 | 00m15s | \u001b[35m   0.83400\u001b[0m | \u001b[32m     9.9749\u001b[0m | \u001b[32m      100.1557\u001b[0m | \u001b[32m     0.5398\u001b[0m | \n",
      "   10 | 00m13s |    0.82499 |      9.9281 |       108.6660 |      0.5042 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11 | 00m18s |    0.82611 |      9.9871 |       162.2832 |      0.5028 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  1.39552144e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 55, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12 | 00m12s |    0.83399 |      9.9851 |       100.0144 |      0.7739 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-0.00038195]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 54, 'nit': 3, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   13 | 00m11s |    0.83286 |      5.7301 |       199.9325 |      0.5199 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -2.74322954e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 64, 'nit': 6, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14 | 00m13s |    0.82836 |      8.3839 |       146.9085 |      0.5016 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   15 | 00m10s |    0.83285 |      9.9956 |       100.1761 |      0.5079 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-0.00014978]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 52, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-0.00011452]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 61, 'nit': 7, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -3.47648715e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 74, 'nit': 7, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16 | 00m10s |    0.82160 |      3.0000 |       173.3162 |      0.5000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   17 | 00m10s |    0.81599 |      3.0090 |       138.6388 |      0.9980 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  8.55310886e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 51, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18 | 00m10s |    0.81599 |      3.0000 |       111.3497 |      0.9477 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  8.54847717e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 49, 'nit': 3, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -2.97668516e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 53, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   19 | 00m16s |    0.82500 |     10.0000 |       121.2577 |      1.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -5.82025025e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 56, 'nit': 4, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 | 00m17s |    0.82949 |      9.9535 |       173.7189 |      0.5260 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  6.84811166e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 60, 'nit': 6, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   21 | 00m18s |    0.82723 |      6.1153 |       104.0760 |      0.9950 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-0.00022745]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 61, 'nit': 8, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22 | 00m13s |    0.82160 |      3.0262 |       164.8094 |      0.9483 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-0.00148365]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 62, 'nit': 6, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -1.34278525e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 54, 'nit': 7, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   23 | 00m14s |    0.82723 |      6.2796 |       116.2670 |      0.5026 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ 0.00131842]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 53, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   24 | 00m14s |    0.82390 |     10.0000 |       193.6389 |      1.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ 0.00148869]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 53, 'nit': 6, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -3.32520358e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 56, 'nit': 6, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   25 | 00m17s |    0.82612 |      9.9524 |       153.9991 |      0.9455 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ 0.00242606]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 73, 'nit': 7, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   26 | 00m18s |    0.82047 |      3.0000 |       181.2080 |      1.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([  4.32782836e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 52, 'nit': 6, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   27 | 00m13s |    0.81487 |      3.0000 |       147.0528 |      1.0000 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-0.00014757]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 51, 'nit': 3, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([-0.00010481]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 58, 'nit': 6, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -6.50478485e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 52, 'nit': 4, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   28 | 00m15s |    0.83174 |      9.9564 |       140.9409 |      0.9997 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29 | 00m14s |    0.83173 |      6.6020 |       100.0170 |      0.6700 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:427: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'grad': array([ -6.41721815e-05]), 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 65, 'nit': 5, 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py:308: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   30 | 00m16s |    0.82160 |      3.1253 |       131.3381 |      0.9746 | \n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(learning_rate=0.1, silent=True, objective='binary:logistic')\n",
    "\n",
    "# def xgb_evaluate(n_estimators, max_depth, subsample): \n",
    "#     \n",
    "#     params['n_estimators'] = int(n_estimators)\n",
    "#     params['max_depth'] = int(max_depth)\n",
    "#     params['subsample'] = max(min(subsample, 1), 0)\n",
    "#     \n",
    "#     cv_result = xgb.cv(params, xgtrain, num_boost_round=num_rounds, nfold=5,\n",
    "#              seed=random_state,\n",
    "#              callbacks=[xgb.callback.early_stop(50)])\n",
    "#     \n",
    "#     return -cv_result['test-mae-mean'].values[-1]\n",
    "\n",
    "# params = {\n",
    "#     'eta': 0.1,\n",
    "#     'silent': 1,\n",
    "#     'eval_metric': 'mae',\n",
    "#     'verbose_eval': True,\n",
    "#     'seed': random_state\n",
    "# }\n",
    "\n",
    "def xgb_evaluate(n_estimators, max_depth, subsample): \n",
    "    \n",
    "    n_estimators= int(n_estimators)\n",
    "    max_depth= int(max_depth)\n",
    "    subsample = max(min(subsample, 1), 0)\n",
    "    \n",
    "    clf = xgb.XGBClassifier(learning_rate=0.1, n_estimators=n_estimators, \\\n",
    "                            max_depth=max_depth, subsample=subsample, \\\n",
    "                            silent=True, objective='binary:logistic')\n",
    "    \n",
    "    cv_scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    \n",
    "    return cross_val_score(clf, X_train, y_train, cv=5).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xgbBO = BayesianOptimization(xgb_evaluate, {'n_estimators': (100, 200),\n",
    "                                            'max_depth': (3, 10),\n",
    "                                            'subsample': (0.5, 1)\n",
    "                                            })\n",
    "num_iter = 25\n",
    "init_points = 5\n",
    "\n",
    "xgbBO.maximize(init_points=init_points, n_iter=num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9.9748670659455332,\n",
       " 'n_estimators': 100.15571535878696,\n",
       " 'subsample': 0.53975673408943159}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbBO.res['max']['max_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83400111426856005"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbBO.res['max']['max_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search\n",
    "clf = grid_search.best_estimator_\n",
    "\n",
    "# # bayesian optimization\n",
    "# clf = xgb.XGBClassifier(max_depth=int(xgbBO.res['max']['max_params']['max_depth']), \\\n",
    "#                        n_estimators=int(xgbBO.res['max']['max_params']['n_estimators']), \\\n",
    "#                        subsample=xgbBO.res['max']['max_params']['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "       min_child_weight=1, missing=None, n_estimators=190, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append new column of tartget column\n",
    "data_test['Survived'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute missing values with median value on each (Pclass, Sex) group for Age column\n",
    "data_test = age_group_by_imputer.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute missing values with median value on each (Pclass, Embarked) group for Fare column\n",
    "data_test = fare_group_by_imputer.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine SibSp and Parch and passenger as Fsize(Family Size)\n",
    "data_test['Fsize'] = data_test['SibSp'] + data_test['Parch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Title from Name\n",
    "data_test = generate_title(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature to divide Age into 'child' and 'adult'\n",
    "data_test['Age_New'] = 'adult'\n",
    "data_test.loc[data_test['Age'] < 18, 'Age_New'] = 'child'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform test data\n",
    "data_test = ohe.transform(data_test)\n",
    "# Remove columns with name containing '-1'(all 0)\n",
    "data_test = data_test[[c for c in data_test.columns if '-1' not in c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for categorical_column in categorical_variables:\n",
    "#     lb = LabelBinarizerDict[categorical_column]\n",
    "#     \n",
    "#     # dummy dataframe \n",
    "#     dummy_df = pd.DataFrame(lb.transform(data_test[categorical_column]), index=None)\n",
    "#     \n",
    "#     # column names of dummy variables\n",
    "#     dummy_column_names = [categorical_column + '_' + str(lb_class) for lb_class in list(lb.classes_)]\n",
    "#     if len(dummy_column_names) == 2:\n",
    "#         dummy_column_names = [dummy_column_names[0]]\n",
    "#     # assign column names to dummy dataframe\n",
    "#     dummy_df.columns = dummy_column_names\n",
    "#     \n",
    "#     data_test = pd.concat([data_test, dummy_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test = data_test[dummy_categorical_variables + numerical_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random forest\n",
    "y_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## submit the result\n",
    "pd.DataFrame({'PassengerId':data_test.PassengerId, 'survived':y_test}).to_csv('solution.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
